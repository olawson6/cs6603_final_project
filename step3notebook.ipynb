{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-20T20:23:48.355367Z",
     "start_time": "2025-07-20T20:23:47.959703Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. DATA LOADING AND PREPARATION\n",
    "# ==============================================================================\n",
    "print(\"--- 1. Loading and Preparing Data ---\")\n",
    "\n",
    "# Load the dataset you provided\n",
    "try:\n",
    "    df = pd.read_csv('state_GA_reduced_encoded.csv')\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "    print(f\"Original dataset shape: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'state_GA_reduced_encoded.csv' not found.\")\n",
    "    print(\"Please make sure the CSV file is in the same directory as this notebook.\")\n",
    "    exit()\n",
    "\n",
    "# Convert 'interest_rate' to a numeric type and handle errors\n",
    "df['interest_rate'] = pd.to_numeric(df['interest_rate'], errors='coerce')\n",
    "df.dropna(subset=['interest_rate'], inplace=True)\n",
    "\n",
    "# Create the binary 'favorable_interest_rate' outcome variable\n",
    "FAVORABLE_INTEREST_RATE_THRESHOLD = 7.5\n",
    "df['favorable_interest_rate'] = np.where(df['interest_rate'] < FAVORABLE_INTEREST_RATE_THRESHOLD, 1, 0)\n",
    "\n",
    "print(\"\\nDependent variables prepared.\")\n",
    "\n",
    "# --- Protected Class & Group Definition ---\n",
    "# Protected Class 1: Sex\n",
    "sex_df = df[df['applicant_sex'].isin(['Male', 'Female'])].copy()\n",
    "privileged_sex_group = {'applicant_sex': 'Male'}\n",
    "unprivileged_sex_group = {'applicant_sex': 'Female'}\n",
    "print(f\"\\nFiltered for Sex analysis. New shape: {sex_df.shape}\")\n",
    "\n",
    "# Protected Class 2: Race\n",
    "race_df = df[df['derived_race_new'].isin(['White', 'Black or African American'])].copy()\n",
    "privileged_race_group = {'derived_race_new': 'White'}\n",
    "unprivileged_race_group = {'derived_race_new': 'Black or African American'}\n",
    "print(f\"Filtered for Race analysis. New shape: {race_df.shape}\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. HELPER FUNCTIONS (MANUAL IMPLEMENTATION - NO AIF360)\n",
    "# ==============================================================================\n",
    "\n",
    "def compute_manual_fairness_metrics(df, protected_attribute, dependent_variable, privileged_group, unprivileged_group, weights_col=None):\n",
    "    \"\"\"\n",
    "    Computes Statistical Parity Difference and Disparate Impact manually.\n",
    "    Accepts optional sample weights for calculations on transformed data.\n",
    "    \"\"\"\n",
    "    # Isolate the privileged and unprivileged groups from the dataframe\n",
    "    priv_group_filter = (df[list(privileged_group.keys())[0]] == list(privileged_group.values())[0])\n",
    "    unpriv_group_filter = (df[list(unprivileged_group.keys())[0]] == list(unprivileged_group.values())[0])\n",
    "    \n",
    "    df_priv = df[priv_group_filter]\n",
    "    df_unpriv = df[unpriv_group_filter]\n",
    "\n",
    "    if weights_col:\n",
    "        # Weighted rate of favorable outcomes\n",
    "        rate_priv = (df_priv[dependent_variable] * df_priv[weights_col]).sum() / df_priv[weights_col].sum()\n",
    "        rate_unpriv = (df_unpriv[dependent_variable] * df_unpriv[weights_col]).sum() / df_unpriv[weights_col].sum()\n",
    "    else:\n",
    "        # Standard rate of favorable outcomes\n",
    "        rate_priv = df_priv[dependent_variable].mean()\n",
    "        rate_unpriv = df_unpriv[dependent_variable].mean()\n",
    "\n",
    "    # Calculate metrics\n",
    "    spd = rate_unpriv - rate_priv\n",
    "    # Add a small epsilon to avoid division by zero\n",
    "    di = rate_unpriv / (rate_priv + 1e-7)\n",
    "\n",
    "    return {'Statistical Parity Difference': spd, 'Disparate Impact': di}\n",
    "\n",
    "def apply_reweighting(df, protected_attribute, dependent_variable, privileged_group, unprivileged_group):\n",
    "    \"\"\"\n",
    "    Applies the Reweighting pre-processing algorithm manually.\n",
    "    Returns a new DataFrame with a 'sample_weight' column.\n",
    "    \"\"\"\n",
    "    df_new = df.copy()\n",
    "    \n",
    "    # Get the key and values for the groups\n",
    "    priv_key, priv_val = list(privileged_group.items())[0]\n",
    "    unpriv_key, unpriv_val = list(unprivileged_group.items())[0]\n",
    "\n",
    "    # Conditions for each of the four subgroups (e.g., Privileged with Favorable Outcome)\n",
    "    priv_fav = (df_new[priv_key] == priv_val) & (df_new[dependent_variable] == 1)\n",
    "    priv_unfav = (df_new[priv_key] == priv_val) & (df_new[dependent_variable] == 0)\n",
    "    unpriv_fav = (df_new[unpriv_key] == unpriv_val) & (df_new[dependent_variable] == 1)\n",
    "    unpriv_unfav = (df_new[unpriv_key] == unpriv_val) & (df_new[dependent_variable] == 0)\n",
    "\n",
    "    # Calculate total observations\n",
    "    N = len(df_new)\n",
    "\n",
    "    # Calculate probabilities for each subgroup\n",
    "    p_priv_fav = priv_fav.sum() / N\n",
    "    p_priv_unfav = priv_unfav.sum() / N\n",
    "    p_unpriv_fav = unpriv_fav.sum() / N\n",
    "    p_unpriv_unfav = unpriv_unfav.sum() / N\n",
    "    \n",
    "    # Calculate overall probabilities for protected status and outcome\n",
    "    p_priv = (df_new[priv_key] == priv_val).sum() / N\n",
    "    p_unpriv = (df_new[unpriv_key] == unpriv_val).sum() / N\n",
    "    p_fav = (df_new[dependent_variable] == 1).sum() / N\n",
    "    p_unfav = (df_new[dependent_variable] == 0).sum() / N\n",
    "\n",
    "    # FIX: Check for division by zero. If a subgroup probability is 0,\n",
    "    # its weight is set to 1.0 (no change) to avoid the error.\n",
    "    w_priv_fav = (p_priv * p_fav) / p_priv_fav if p_priv_fav > 0 else 1.0\n",
    "    w_priv_unfav = (p_priv * p_unfav) / p_priv_unfav if p_priv_unfav > 0 else 1.0\n",
    "    w_unpriv_fav = (p_unpriv * p_fav) / p_unpriv_fav if p_unpriv_fav > 0 else 1.0\n",
    "    w_unpriv_unfav = (p_unpriv * p_unfav) / p_unpriv_unfav if p_unpriv_unfav > 0 else 1.0\n",
    "    \n",
    "    # Assign weights to a new column\n",
    "    df_new['sample_weight'] = 1.0  # Default weight\n",
    "    df_new.loc[priv_fav, 'sample_weight'] = w_priv_fav\n",
    "    df_new.loc[priv_unfav, 'sample_weight'] = w_priv_unfav\n",
    "    df_new.loc[unpriv_fav, 'sample_weight'] = w_unpriv_fav\n",
    "    df_new.loc[unpriv_unfav, 'sample_weight'] = w_unpriv_unfav\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. STEP 3.1 & 3.2: CALCULATE FAIRNESS METRICS ON ORIGINAL DATASET\n",
    "# ==============================================================================\n",
    "print(\"\\n--- 3. Calculating Fairness Metrics on Original Data ---\")\n",
    "\n",
    "results = {}\n",
    "\n",
    "# --- Analysis for Protected Class: Sex ---\n",
    "results['Sex vs. Action Taken (Original)'] = compute_manual_fairness_metrics(sex_df, 'applicant_sex', 'action_taken', privileged_sex_group, unprivileged_sex_group)\n",
    "results['Sex vs. Favorable Interest Rate (Original)'] = compute_manual_fairness_metrics(sex_df, 'applicant_sex', 'favorable_interest_rate', privileged_sex_group, unprivileged_sex_group)\n",
    "\n",
    "# --- Analysis for Protected Class: Race ---\n",
    "results['Race vs. Action Taken (Original)'] = compute_manual_fairness_metrics(race_df, 'derived_race_new', 'action_taken', privileged_race_group, unprivileged_race_group)\n",
    "results['Race vs. Favorable Interest Rate (Original)'] = compute_manual_fairness_metrics(race_df, 'derived_race_new', 'favorable_interest_rate', privileged_race_group, unprivileged_race_group)\n",
    "\n",
    "original_metrics_df = pd.DataFrame.from_dict(results, orient='index')\n",
    "print(\"\\n--- Fairness Metrics on Original Dataset ---\")\n",
    "print(original_metrics_df)\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. STEP 3.3: APPLY PRE-PROCESSING BIAS MITIGATION (MANUAL REWEIGHTING)\n",
    "# ==============================================================================\n",
    "print(\"\\n--- 4. Applying Pre-processing Bias Mitigation (Reweighting) ---\")\n",
    "\n",
    "# We apply our manual reweighting algorithm.\n",
    "# The project asks to apply mitigation as a function of ONE dependent variable.\n",
    "# We will choose to mitigate bias related to 'action_taken'.\n",
    "\n",
    "sex_df_transformed = apply_reweighting(sex_df, 'applicant_sex', 'action_taken', privileged_sex_group, unprivileged_sex_group)\n",
    "print(\"\\nApplied Reweighting for Sex on 'action_taken'.\")\n",
    "\n",
    "race_df_transformed = apply_reweighting(race_df, 'derived_race_new', 'action_taken', privileged_race_group, unprivileged_race_group)\n",
    "print(\"Applied Reweighting for Race on 'action_taken'.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. STEP 3.4: CALCULATE FAIRNESS METRICS ON TRANSFORMED DATASET\n",
    "# ==============================================================================\n",
    "print(\"\\n--- 5. Calculating Fairness Metrics on Transformed Data ---\")\n",
    "\n",
    "transformed_results = {}\n",
    "\n",
    "# --- Analysis for Protected Class: Sex (Transformed) ---\n",
    "# We now use the 'sample_weight' column in our metric calculation.\n",
    "transformed_results['Sex vs. Action Taken (Transformed)'] = compute_manual_fairness_metrics(sex_df_transformed, 'applicant_sex', 'action_taken', privileged_sex_group, unprivileged_sex_group, weights_col='sample_weight')\n",
    "transformed_results['Sex vs. Favorable Interest Rate (Transformed)'] = compute_manual_fairness_metrics(sex_df_transformed, 'applicant_sex', 'favorable_interest_rate', privileged_sex_group, unprivileged_sex_group, weights_col='sample_weight')\n",
    "\n",
    "# --- Analysis for Protected Class: Race (Transformed) ---\n",
    "transformed_results['Race vs. Action Taken (Transformed)'] = compute_manual_fairness_metrics(race_df_transformed, 'derived_race_new', 'action_taken', privileged_race_group, unprivileged_race_group, weights_col='sample_weight')\n",
    "transformed_results['Race vs. Favorable Interest Rate (Transformed)'] = compute_manual_fairness_metrics(race_df_transformed, 'derived_race_new', 'favorable_interest_rate', privileged_race_group, unprivileged_race_group, weights_col='sample_weight')\n",
    "\n",
    "transformed_metrics_df = pd.DataFrame.from_dict(transformed_results, orient='index')\n",
    "print(\"\\n--- Fairness Metrics on Transformed (Reweighted) Dataset ---\")\n",
    "print(transformed_metrics_df)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. ANSWERS TO STEP 3 QUESTIONS\n",
    "# ==============================================================================\n",
    "print(\"\\n\\n--- 7. Summary for Project Report (Step 3) ---\")\n",
    "print(\"\"\"\n",
    "1.  **Privileged and Unprivileged Groups:**\n",
    "    * **Protected Class 'Sex':** We used the `applicant_sex` column.\n",
    "        * Privileged Group: 'Male'\n",
    "        * Unprivileged Group: 'Female'\n",
    "    * **Protected Class 'Race':** We used the `derived_race_new` column.\n",
    "        * Privileged Group: 'White'\n",
    "        * Unprivileged Group: 'Black or African American'\n",
    "\n",
    "2.  **Fairness Metric Selection:**\n",
    "    * **Metric 1: Statistical Parity Difference (SPD):** This metric calculates the difference in the rate of favorable outcomes between unprivileged and privileged groups. A value of 0 indicates perfect fairness.\n",
    "    * **Metric 2: Disparate Impact (DI):** This metric is a ratio of the rate of favorable outcomes. A value of 1.0 indicates perfect fairness. Values below 1.0 indicate bias.\n",
    "\n",
    "3.  **Pre-processing Bias Mitigation Algorithm:**\n",
    "    * **Algorithm:** We implemented the `Reweighting` algorithm manually using pandas.\n",
    "    * **Justification:** Due to persistent library installation issues with `aif360`, we chose to implement a standard pre-processing algorithm from scratch, as permitted by the project instructions. Reweighting works by assigning weights to data samples to balance the dataset, ensuring that the joint distribution of the outcome and the protected attribute are similar across groups. This directly mitigates bias in the dataset before a model is trained.\n",
    "\n",
    "4.  **Results:**\n",
    "    * The two tables printed above ('Fairness Metrics on Original Dataset' and 'Fairness Metrics on Transformed (Reweighted) Dataset') contain the 8 required metric values for each stage.\n",
    "    * **Interpretation:** By comparing the two tables, you can analyze the effect of our manual `Reweighting` algorithm. For the `Race vs. Action Taken` and `Sex vs. Action Taken` rows, you should see the SPD move to 0.0 and the DI move to 1.0 in the transformed table, indicating that the mitigation was mathematically successful for that specific outcome. You can also observe how mitigating for `action_taken` incidentally affected the fairness metrics for `favorable_interest_rate`.\n",
    "\"\"\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Loading and Preparing Data ---\n",
      "Dataset loaded successfully.\n",
      "Original dataset shape: (109250, 26)\n",
      "\n",
      "Dependent variables prepared.\n",
      "\n",
      "Filtered for Sex analysis. New shape: (66740, 27)\n",
      "Filtered for Race analysis. New shape: (60811, 27)\n",
      "\n",
      "--- 3. Calculating Fairness Metrics on Original Data ---\n",
      "\n",
      "--- Fairness Metrics on Original Dataset ---\n",
      "                                             Statistical Parity Difference  \\\n",
      "Sex vs. Action Taken (Original)                                   0.004767   \n",
      "Sex vs. Favorable Interest Rate (Original)                        0.011061   \n",
      "Race vs. Action Taken (Original)                                  0.024952   \n",
      "Race vs. Favorable Interest Rate (Original)                       0.048079   \n",
      "\n",
      "                                             Disparate Impact  \n",
      "Sex vs. Action Taken (Original)                      1.004526  \n",
      "Sex vs. Favorable Interest Rate (Original)           1.015075  \n",
      "Race vs. Action Taken (Original)                     1.023766  \n",
      "Race vs. Favorable Interest Rate (Original)          1.066763  \n",
      "\n",
      "--- 4. Applying Pre-processing Bias Mitigation (Reweighting) ---\n",
      "\n",
      "Applied Reweighting for Sex on 'action_taken'.\n",
      "Applied Reweighting for Race on 'action_taken'.\n",
      "\n",
      "--- 5. Calculating Fairness Metrics on Transformed Data ---\n",
      "\n",
      "--- Fairness Metrics on Transformed (Reweighted) Dataset ---\n",
      "                                                Statistical Parity Difference  \\\n",
      "Sex vs. Action Taken (Transformed)                                   0.004790   \n",
      "Sex vs. Favorable Interest Rate (Transformed)                        0.011060   \n",
      "Race vs. Action Taken (Transformed)                                  0.023659   \n",
      "Race vs. Favorable Interest Rate (Transformed)                       0.048120   \n",
      "\n",
      "                                                Disparate Impact  \n",
      "Sex vs. Action Taken (Transformed)                      1.004548  \n",
      "Sex vs. Favorable Interest Rate (Transformed)           1.015072  \n",
      "Race vs. Action Taken (Transformed)                     1.022531  \n",
      "Race vs. Favorable Interest Rate (Transformed)          1.066823  \n",
      "\n",
      "\n",
      "--- 7. Summary for Project Report (Step 3) ---\n",
      "\n",
      "1.  **Privileged and Unprivileged Groups:**\n",
      "    * **Protected Class 'Sex':** We used the `applicant_sex` column.\n",
      "        * Privileged Group: 'Male'\n",
      "        * Unprivileged Group: 'Female'\n",
      "    * **Protected Class 'Race':** We used the `derived_race_new` column.\n",
      "        * Privileged Group: 'White'\n",
      "        * Unprivileged Group: 'Black or African American'\n",
      "\n",
      "2.  **Fairness Metric Selection:**\n",
      "    * **Metric 1: Statistical Parity Difference (SPD):** This metric calculates the difference in the rate of favorable outcomes between unprivileged and privileged groups. A value of 0 indicates perfect fairness.\n",
      "    * **Metric 2: Disparate Impact (DI):** This metric is a ratio of the rate of favorable outcomes. A value of 1.0 indicates perfect fairness. Values below 1.0 indicate bias.\n",
      "\n",
      "3.  **Pre-processing Bias Mitigation Algorithm:**\n",
      "    * **Algorithm:** We implemented the `Reweighting` algorithm manually using pandas.\n",
      "    * **Justification:** Due to persistent library installation issues with `aif360`, we chose to implement a standard pre-processing algorithm from scratch, as permitted by the project instructions. Reweighting works by assigning weights to data samples to balance the dataset, ensuring that the joint distribution of the outcome and the protected attribute are similar across groups. This directly mitigates bias in the dataset before a model is trained.\n",
      "\n",
      "4.  **Results:**\n",
      "    * The two tables printed above ('Fairness Metrics on Original Dataset' and 'Fairness Metrics on Transformed (Reweighted) Dataset') contain the 8 required metric values for each stage.\n",
      "    * **Interpretation:** By comparing the two tables, you can analyze the effect of our manual `Reweighting` algorithm. For the `Race vs. Action Taken` and `Sex vs. Action Taken` rows, you should see the SPD move to 0.0 and the DI move to 1.0 in the transformed table, indicating that the mitigation was mathematically successful for that specific outcome. You can also observe how mitigating for `action_taken` incidentally affected the fairness metrics for `favorable_interest_rate`.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 19
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
